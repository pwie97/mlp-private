{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lung and Colon Cancer Detection\n",
    "This notebook demonstrates the process of building a machine learning model to detect lung and colon cancer using histopathological images. The dataset contains labeled images of cancerous and non-cancerous (<em>healthy</em>) tissues. \n",
    "\n",
    "There are five classes in this dataset: \n",
    "- Lung benign tissue (<em>healthy</em>)\n",
    "- Lung adenocarcinoma\n",
    "- Lung squamos cell carcinoma\n",
    "- Colon adenocarcinoma\n",
    "- Colong benign tissue (<em>healthy</em>)\n",
    "\n",
    "The goal is to compare different Convolutional Neural Networks (CNNs) to explore the strengths and weaknesses of various architectures and understand which ones perform best for the chosen dataset. Ultimately, the most robust classifier (CNN) will be identified and can accurately identify cancerous lung or colon tissues from the given sample images. \n",
    "\n",
    "For a fair comparison of the different CNNs, it is necessary to set some guidelines / rules:\n",
    "- The dataset has to be properly preprocessed.\n",
    "- The same training parameters are used \n",
    "  - Learning rate\n",
    "  - Batch size\n",
    "  - Number of epochs\n",
    "- The same optimizer is used\n",
    "  - Isolates the effect if the CNN architecture on performance\n",
    "\n",
    "Those rules will ensure that the comparison is consistent, controlled and fair.\n",
    "\n",
    "The dataset used in this notebook is sourced from Kaggle: https://www.kaggle.com/datasets/andrewmvd/lung-and-colon-cancer-histopathological-images/data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset handling\n",
    "### Import Dataset\n",
    "The dataset containes 25.000 images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes found in lung dataset: ['.DS_Store', 'lung_aca', 'lung_n', 'lung_scc']\n",
      "Classes found in colon dataset: ['colon_aca', 'colon_n']\n",
      "Lung and Colon classes combined: ['.DS_Store', 'colon_aca', 'colon_n', 'lung_aca', 'lung_n', 'lung_scc']\n"
     ]
    }
   ],
   "source": [
    "lung_dataset = '../lung_colon_image_set/lung_image_sets'\n",
    "colon_dataset = '../lung_colon_image_set/colon_image_sets'\n",
    "\n",
    "if not os.path.exists(lung_dataset):\n",
    "    raise FileNotFoundError(f\"Dataset path '{lung_dataset}' does not exist!\")\n",
    "if not os.path.exists(colon_dataset):\n",
    "    raise FileNotFoundError(f\"Dataset path '{colon_dataset}' does not exist!\")\n",
    "\n",
    "lung_classes = sorted(os.listdir(lung_dataset))\n",
    "colon_classes = sorted(os.listdir(colon_dataset))\n",
    "\n",
    "print(f\"Classes found in lung dataset: {lung_classes}\")\n",
    "print(f\"Classes found in colon dataset: {colon_classes}\")\n",
    "\n",
    "lc_classes = sorted(set(lung_classes + colon_classes))\n",
    "print(f\"Lung and Colon classes combined: {lc_classes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Dataset\n",
    "The dataset is split into the following three categories with pre defined percentages:\n",
    "- Training data (<em>80 %</em>)\n",
    "- Validation data (<em>10 %</em>)\n",
    "- Testing data (<em>10 %</em>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 'lung_aca' has 5000 images\n",
      "Class 'lung_n' has 5000 images\n",
      "Class 'lung_scc' has 5000 images\n",
      "Class 'colon_aca' has 5000 images\n",
      "Class 'colon_n' has 5000 images\n",
      "Training dataset has 20000 images\n",
      "Validation dataset has 2500 images\n",
      "Testing dataset has 2500 images\n"
     ]
    }
   ],
   "source": [
    "def prepare_splits(data_directories, split_ratios=(0.8, 0.1, 0.1)):\n",
    "    all_data = {}\n",
    "    for data_directory in data_directories:\n",
    "        for class_name in sorted(os.listdir(data_directory)):\n",
    "            class_directory = os.path.join(data_directory, class_name)\n",
    "            if os.path.isdir(class_directory):\n",
    "                all_data.setdefault(class_name, []).extend(os.path.join(class_directory, file_name) for file_name in os.listdir(class_directory))\n",
    "\n",
    "    for class_name, files in all_data.items():\n",
    "        print(f\"Class '{class_name}' has {len(files)} images\")\n",
    "\n",
    "    dataset_splits = {'training': [], 'validation': [], 'testing': []}\n",
    "\n",
    "    for class_name, files in all_data.items():\n",
    "        training_dataset, temporary_dataset = train_test_split(files, test_size=(1 - split_ratios[0]), random_state=42)\n",
    "\n",
    "        validation_dataset, testing_dataset = train_test_split(temporary_dataset, test_size=split_ratios[2]/(split_ratios[1] + split_ratios[2]), random_state=42)\n",
    "\n",
    "        dataset_splits['training'].extend(training_dataset)\n",
    "        dataset_splits['validation'].extend(validation_dataset)\n",
    "        dataset_splits['testing'].extend(testing_dataset)\n",
    "\n",
    "    return dataset_splits\n",
    "\n",
    "dataset_directories = [lung_dataset, colon_dataset]\n",
    "dataset_splits = prepare_splits(dataset_directories)\n",
    "\n",
    "print(f\"Training dataset has {len(dataset_splits['training'])} images\")\n",
    "print(f\"Validation dataset has {len(dataset_splits['validation'])} images\")\n",
    "print(f\"Testing dataset has {len(dataset_splits['testing'])} images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Class\n",
    "Initializes the dataset class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CancerDetectionDataset(Dataset):\n",
    "    def __init__(self, file_paths, all_classes, transform=None):\n",
    "        self.file_paths = file_paths\n",
    "        self.all_classes = all_classes\n",
    "        self.labels = [all_classes.index(os.path.basename(os.path.dirname(file_path))) for file_path in file_paths]\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.file_paths[idx]\n",
    "        label = self.labels[idx]\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean and Standard Deviation\n",
    "Overall, normalization and standardization helps in stabilizing and speeding up the training process of machine learning models.\n",
    "\n",
    "These are the main reasons:\n",
    "- Subtracting the mean from each image centers the data around zero.\n",
    "- Dividing by the standard deviation scales the data to have unit variance.\n",
    "- Normalized inputs can lead to faster and improved convergance during training because the gradients are more stable and the optimization is more efficient.\n",
    "- Normalization ensures that all input features have are within the same, consistent range.\n",
    "\n",
    "Calculating the mean and standard deviation of the specific dataset, rather than using standard values, is quite helpful:\n",
    "- Dataset specificity,\n",
    "- Improved model performance,\n",
    "- Avoiding bias and\n",
    "- Consistency\n",
    "\n",
    "By calculating the mean and standard deviation specific to the dataset, it is ensured that the normalization is optimal for the data, leading to better model performance and more reliable results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: tensor([0.6440, 0.5290, 0.7740])\n",
      "Std: tensor([0.2620, 0.2520, 0.2800])\n"
     ]
    }
   ],
   "source": [
    "def get_image_paths(dataset_splits):\n",
    "    keys = ['training', 'validation', 'testing']\n",
    "    image_paths = []\n",
    "\n",
    "    for key in keys:\n",
    "        image_paths.extend(dataset_splits[key])\n",
    "    \n",
    "    return image_paths\n",
    "\n",
    "transformms = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.RandomRotation(degrees=(-45, 45)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "def calculate_mean_std(loader):\n",
    "    mean = torch.zeros(3)\n",
    "    std = torch.zeros(3)\n",
    "    total_images_count = 0\n",
    "    for images, _ in loader:\n",
    "        batch_samples = images.size(0)\n",
    "        images = images.view(batch_samples, images.size(1), -1)\n",
    "        mean += images.mean(2).sum(0)\n",
    "        std += images.std(2).sum(0)\n",
    "        total_images_count += batch_samples\n",
    "\n",
    "    mean /= total_images_count\n",
    "    std /= total_images_count\n",
    "    \n",
    "    return mean, std\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    dataset = CancerDetectionDataset(get_image_paths(dataset_splits), lc_classes, transform=transformms)\n",
    "    loader = DataLoader(dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "    mean, std = calculate_mean_std(loader)\n",
    "    print(f'Mean: {torch.round(mean, decimals=3)}')\n",
    "    print(f'Std: {torch.round(std, decimals=3)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer\n",
    "Transforms the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.RandomRotation(degrees=(-45, 45)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.644, 0.529, 0.774], std=[0.262, 0.252, 0.280])\n",
    "])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
