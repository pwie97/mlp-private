{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lung and Colon Cancer Detection\n",
    "This notebook demonstrates the process of building a machine learning model to detect lung and colon cancer using histopathological images. The dataset contains labeled images of cancerous and non-cancerous (<em>healthy</em>) tissues. \n",
    "\n",
    "There are five classes in this dataset: \n",
    "- Lung benign tissue (<em>healthy</em>)\n",
    "- Lung adenocarcinoma\n",
    "- Lung squamos cell carcinoma\n",
    "- Colon adenocarcinoma\n",
    "- Colong benign tissue (<em>healthy</em>)\n",
    "\n",
    "The goal is to compare different Convolutional Neural Networks (CNNs) to explore the strengths and weaknesses of various architectures and understand which ones perform best for the chosen dataset. Ultimately, the most robust classifier (CNN) will be identified and can accurately identify cancerous lung or colon tissues from the given sample images. \n",
    "\n",
    "For a fair comparison of the different CNNs, it is necessary to set some guidelines / rules:\n",
    "- The dataset has to be properly preprocessed.\n",
    "- The same training parameters are used \n",
    "  - Learning rate\n",
    "  - Batch size\n",
    "  - Number of epochs\n",
    "- The same optimizer is used\n",
    "  - Isolates the effect if the CNN architecture on performance\n",
    "\n",
    "Those rules will ensure that the comparison is consistent, controlled and fair.\n",
    "\n",
    "The dataset used in this notebook is sourced from Kaggle: https://www.kaggle.com/datasets/andrewmvd/lung-and-colon-cancer-histopathological-images/data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Handling\n",
    "### Import Dataset\n",
    "The dataset containes 25.000 images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lung_dataset = '../lung_colon_image_set/lung_image_sets'\n",
    "colon_dataset = '../lung_colon_image_set/colon_image_sets'\n",
    "\n",
    "if not os.path.exists(lung_dataset):\n",
    "    raise FileNotFoundError(f\"Dataset path '{lung_dataset}' does not exist!\")\n",
    "if not os.path.exists(colon_dataset):\n",
    "    raise FileNotFoundError(f\"Dataset path '{colon_dataset}' does not exist!\")\n",
    "\n",
    "lung_classes = sorted(os.listdir(lung_dataset))\n",
    "colon_classes = sorted(os.listdir(colon_dataset))\n",
    "\n",
    "print(f\"Classes found in lung dataset: {lung_classes}\")\n",
    "print(f\"Classes found in colon dataset: {colon_classes}\")\n",
    "\n",
    "lc_classes = sorted(set(lung_classes + colon_classes))\n",
    "print(f\"Lung and Colon classes combined: {lc_classes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Dataset\n",
    "The dataset is split into the following three categories with pre defined percentages:\n",
    "- Training data (<em>80 %</em>)\n",
    "- Validation data (<em>10 %</em>)\n",
    "- Testing data (<em>10 %</em>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_splits(data_directories, split_ratios=(0.8, 0.1, 0.1)):\n",
    "    all_data = {}\n",
    "    for data_directory in data_directories:\n",
    "        for class_name in sorted(os.listdir(data_directory)):\n",
    "            class_directory = os.path.join(data_directory, class_name)\n",
    "            if os.path.isdir(class_directory):\n",
    "                all_data.setdefault(class_name, []).extend(os.path.join(class_directory, file_name) for file_name in os.listdir(class_directory))\n",
    "\n",
    "    for class_name, files in all_data.items():\n",
    "        print(f\"Class '{class_name}' has {len(files)} images\")\n",
    "\n",
    "    dataset_splits = {'training': [], 'validation': [], 'testing': []}\n",
    "\n",
    "    for class_name, files in all_data.items():\n",
    "        training_dataset, temporary_dataset = train_test_split(files, test_size=(1 - split_ratios[0]), random_state=42)\n",
    "\n",
    "        validation_dataset, testing_dataset = train_test_split(temporary_dataset, test_size=split_ratios[2]/(split_ratios[1] + split_ratios[2]), random_state=42)\n",
    "\n",
    "        dataset_splits['training'].extend(training_dataset)\n",
    "        dataset_splits['validation'].extend(validation_dataset)\n",
    "        dataset_splits['testing'].extend(testing_dataset)\n",
    "\n",
    "    return dataset_splits\n",
    "\n",
    "dataset_directories = [lung_dataset, colon_dataset]\n",
    "dataset_splits = prepare_splits(dataset_directories)\n",
    "\n",
    "print(f\"Training dataset has {len(dataset_splits['training'])} images\")\n",
    "print(f\"Validation dataset has {len(dataset_splits['validation'])} images\")\n",
    "print(f\"Testing dataset has {len(dataset_splits['testing'])} images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Class\n",
    "Initializes the dataset class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CancerDetectionDataset(Dataset):\n",
    "    def __init__(self, file_paths, all_classes, transform=None):\n",
    "        self.file_paths = file_paths\n",
    "        self.all_classes = all_classes\n",
    "        self.labels = [all_classes.index(os.path.basename(os.path.dirname(file_path))) for file_path in file_paths]\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.file_paths[idx]\n",
    "        label = self.labels[idx]\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean and Standard Deviation\n",
    "Overall, normalization and standardization helps in stabilizing and speeding up the training process of machine learning models.\n",
    "\n",
    "These are the main reasons:\n",
    "- Subtracting the mean from each image centers the data around zero.\n",
    "- Dividing by the standard deviation scales the data to have unit variance.\n",
    "- Normalized inputs can lead to faster and improved convergance during training because the gradients are more stable and the optimization is more efficient.\n",
    "- Normalization ensures that all input features have are within the same, consistent range.\n",
    "\n",
    "Calculating the mean and standard deviation of the specific dataset, rather than using standard values, is quite helpful:\n",
    "- Dataset specificity,\n",
    "- Improved model performance,\n",
    "- Avoiding bias and\n",
    "- Consistency\n",
    "\n",
    "By calculating the mean and standard deviation specific to the dataset, it is ensured that the normalization is optimal for the data, leading to better model performance and more reliable results.\n",
    "\n",
    "This step will usually be performed once to determine the mean and standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_paths(dataset_splits):\n",
    "    keys = ['training', 'validation', 'testing']\n",
    "    image_paths = []\n",
    "\n",
    "    for key in keys:\n",
    "        image_paths.extend(dataset_splits[key])\n",
    "    \n",
    "    return image_paths\n",
    "\n",
    "transformms = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.RandomRotation(degrees=(-45, 45)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "def calculate_mean_std(loader):\n",
    "    mean = torch.zeros(3)\n",
    "    std = torch.zeros(3)\n",
    "    total_images_count = 0\n",
    "    for images, _ in loader:\n",
    "        batch_samples = images.size(0)\n",
    "        images = images.view(batch_samples, images.size(1), -1)\n",
    "        mean += images.mean(2).sum(0)\n",
    "        std += images.std(2).sum(0)\n",
    "        total_images_count += batch_samples\n",
    "\n",
    "    mean /= total_images_count\n",
    "    std /= total_images_count\n",
    "    \n",
    "    return mean, std\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    dataset = CancerDetectionDataset(get_image_paths(dataset_splits), lc_classes, transform=transformms)\n",
    "    loader = DataLoader(dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "    mean, std = calculate_mean_std(loader)\n",
    "    print(f'Mean: {torch.round(mean, decimals=3)}')\n",
    "    print(f'Std: {torch.round(std, decimals=3)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer\n",
    "Transforms the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.RandomRotation(degrees=(-45, 45)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.644, 0.529, 0.774], std=[0.262, 0.252, 0.280])\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Classes und Data Loaders\n",
    "Builds the dataset classes and data loadersfor the training, validation and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset = CancerDetectionDataset(dataset_splits['training'], lc_classes, transform=transform)\n",
    "validation_dataset = CancerDetectionDataset(dataset_splits['validation'], lc_classes, transform=transform)\n",
    "testing_dataset = CancerDetectionDataset(dataset_splits['testing'], lc_classes, transform=transform)\n",
    "\n",
    "training_loader = DataLoader(training_dataset, batch_size=16, shuffle=True)\n",
    "validation_loader = DataLoader(validation_dataset, batch_size=16, shuffle=False)\n",
    "testing_loader = DataLoader(testing_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPU Initialization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('mps' if torch.mps.is_available() else 'cpu')\n",
    "print(f\"MPS available: {torch.backends.mps.is_available()}\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN-Models\n",
    "The starting point for this experiment is the deep convolutional neural network model 'VGG-16'. This model is composed of 16 layers (13 convolutional layers & 3 fully connected layers). VVG16 is known for it's simplicity and depth. Furthermore, the model is often used as a benchmark.\n",
    "\n",
    "The '__init__' constructur initializes the layers of the VGG-16 model. Convolutional layers are with batch normalization and ReLU activation functions. Max pooling is applied after layers 2, 4, 7 and 10 to reduce spatial dimensions. The fully connected layers use dropout to prevent overfitting and ReLU activation functions.\n",
    "\n",
    "The 'forward' method defines the forward pass of the model. <br>\n",
    "out = self.layer1(x) <br> \n",
    "-> passes the input x through the first convolutional layer (layer1).out = self.layer2out) <br> \n",
    "-> passes the output of the first layer trough the second convolutional layer (layer2) ... <br>\n",
    "out = out.reshape(out.size(0), -1) -> flattens the output from convolutional layers to a 1D tensor and passes the flattened output through the fully connected layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG16(nn.Module):\n",
    "    def __init__(self, num_classes=5):\n",
    "        super(VGG16, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU())\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU())\n",
    "        self.layer4 = nn.Sequential(\n",
    "            nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.layer5 = nn.Sequential(\n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU())\n",
    "        self.layer6 = nn.Sequential(\n",
    "            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU())\n",
    "        self.layer7 = nn.Sequential(\n",
    "            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.layer8 = nn.Sequential(\n",
    "            nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU())\n",
    "        self.layer9 = nn.Sequential(\n",
    "            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU())\n",
    "        self.layer10 = nn.Sequential(\n",
    "            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.layer11 = nn.Sequential(\n",
    "            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU())\n",
    "        self.layer12 = nn.Sequential(\n",
    "            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU())\n",
    "        self.layer13 = nn.Sequential(\n",
    "            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(7*7*512, 4096),\n",
    "            nn.ReLU())\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU())\n",
    "        self.fc2 = nn.Sequential(\n",
    "            nn.Linear(4096, num_classes))\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = self.layer5(out)\n",
    "        out = self.layer6(out)\n",
    "        out = self.layer7(out)\n",
    "        out = self.layer8(out)\n",
    "        out = self.layer9(out)\n",
    "        out = self.layer10(out)\n",
    "        out = self.layer11(out)\n",
    "        out = self.layer12(out)\n",
    "        out = self.layer13(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        out = self.fc1(out)\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, training_loader, validation_loader, number_epochs = 5, lr=0.001):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    model.to(device)\n",
    "\n",
    "    training_losses = []\n",
    "    validation_losses = []\n",
    "    validation_accuracies = []\n",
    "\n",
    "    for epoch in range(number_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for images, labels in training_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        epoch_training_loss = running_loss / len(training_loader)\n",
    "        training_losses.append(epoch_training_loss)\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{number_epochs}, Training Loss: {running_loss/len(training_loader)}\")\n",
    "\n",
    "        \n",
    "        model.eval()\n",
    "        validation_loss = 0.0\n",
    "        correct_predictions = 0\n",
    "        total_predictions = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for images, labels in validation_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                validation_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total_predictions += labels.size(0)\n",
    "                correct_predictions += (predicted == labels).sum().item()\n",
    "        \n",
    "        epoch_validation_loss = validation_loss / len(validation_loader)\n",
    "        validation_losses.append(epoch_validation_loss)\n",
    "        validation_accuracies.append(validation_accuracy)\n",
    "\n",
    "        validation_accuracy = 100 * correct_predictions / total_predictions\n",
    "\n",
    "        print(f\"Validation Loss: {validation_loss/len(validation_loader)}, Validation Accuracy: {validation_accuracy}\")\n",
    "\n",
    "    return training_losses, validation_losses, validation_accuracies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    model = VGG16(num_classes=len(lc_classes)).to(device)\n",
    "    training_losses, validation_losses, validation_accuracies = train_model(model, training_loader, validation_loader)\n",
    "\n",
    "    print(\"Training completed!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
